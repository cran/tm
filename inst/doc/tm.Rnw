\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{201C}{"}
\DeclareUnicodeCharacter{201D}{"}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\acronym}[1]{\textsc{#1}}

%% \VignetteIndexEntry{Introduction to the tm Package}

\begin{document}
<<echo=FALSE>>=
options(width = 75)
### for sampling
set.seed <- 1234
@
\title{Introduction to the \pkg{tm} Package\\Text Mining in \proglang{R}}
\author{Ingo Feinerer}
\maketitle
\sloppy

\begin{abstract}
This vignette gives a short overview over available features in the
\pkg{tm} package for text mining purposes in \proglang{R}.
\end{abstract}

\section*{Loading the Package}
Before actually working we need to load the package:
<<>>=
library("tm")
@

\section*{Data Import}
The main structure for managing documents is a so-called text document
collection (\class{TextDocCol}). Its constructor takes following arguments:
\begin{itemize}
\item \code{object}: a \class{Source} object which abstracts the input location.
\item \code{readerControl}: a list with the named components
  \code{reader}, \code{language}, and \code{load}.
  A reader constructs a text document from a single element
  delivered by a source. A reader must have the argument signature \code{(elem,
    load, language, id)}. The first argument is the element provided
  from the source, the second gives the text's language, the third
  indicates whether the user wants to load the documents immediately
  into memory, and the fourth is a unique identification string.
  If the passed over \code{reader} object is of
  class~\class{FunctionGenerator}, it is assumed to be a function
  generating a reader. This way custom readers taking various
  parameters (specified in \code{...}) can be built, which in fact
  must produce a valid reader signature but can access additional
  parameters via lexical scoping (i.e., by the including
  environment).
\item \code{dbControl}: a list with the named components \code{useDb}
  indicating that database support should be activated, \code{dbName} giving the
  filename holding the sourced out objects (i.e., the database), and
  \code{dbType} holding a valid database type as supported by
  \code{filehash}. Under activated database
  support the \code{tm} packages tries to keep as few as possible
  resources in memory under usage of the database.
\item \code{...}: Further arguments to the reader.
\end{itemize}

Available sources are \class{DirSource}, \class{CSVSource},
\class{GmaneSource} and
\class{ReutersSource} which handle a directory, a mixed CSV, a Gmane
mailing list archive \acronym{Rss} feed or a
mixed Reuters file (mixed means several documents are in a single
file). Except \class{DirSource}, which is designated
solely for directories on a file system, all other implemented sources
can take connections as input (a character string is interpreted as
file path).

This package ships with several readers (\code{readPlain()}
(default), \code{readRCV1()}, \code{readReut21578XML()},
\code{readGmane()}, \code{readNewsgroup()}, \code{readPDF()}, and \code{readHTML()}).
Each source has a default reader which can be overridden. E.g., for \code{DirSource} the
default just reads in the whole input files and interprets their
content as text.

Plain text files in a directory:
<<keep.source=TRUE>>=
txt <- system.file("texts", "txt", package = "tm")
(ovid <- TextDocCol(DirSource(txt),
                    readerControl = list(reader = readPlain,
                                         language = "en_US",
                                         load = TRUE)))
@

A single comma separated values file:
<<>>=
# Comma separated values
cars <- system.file("texts", "cars.csv", package = "tm")
TextDocCol(CSVSource(cars))
@

Reuters21578 files either in directory (one document per file) or a single
file (several documents per file). Note that connections can be used
as input:
<<keep.source=TRUE>>=
# Reuters21578 XML
reut21578 <- system.file("texts", "reut21578", package = "tm")
reut21578XML <- system.file("texts", "reut21578.xml", package = "tm")
reut21578XMLgz <- system.file("texts", "reut21578.xml.gz", package = "tm")

(reut21578TDC <- TextDocCol(DirSource(reut21578),
                            readerControl = list(reader = readReut21578XML,
                                                 language = "en_US",
                                                 load = FALSE)))

TextDocCol(ReutersSource(reut21578XML),
           readerControl = list(reader = readReut21578XML,
                                language = "en_US", load = FALSE))
TextDocCol(ReutersSource(gzfile(reut21578XMLgz)),
           readerControl = list(reader = readReut21578XML,
                                language = "en_US", load = FALSE))
@
Depending on your exact input format you might find
\code{preprocessReut21578XML()} useful. For the original downloadable
archive this function can correct invalid \acronym{Utf8} encodings and
can copy each text document into a separate file to enable load on
demand.

Analogously we can construct collections for files in the Reuters
Corpus Volume 1 format:
<<>>=
# Reuters Corpus Volume 1
rcv1 <- system.file("texts", "rcv1", package = "tm")
rcv1XML <- system.file("texts", "rcv1.xml", package = "tm")

TextDocCol(DirSource(rcv1),
           readerControl = list(reader = readRCV1, language = "en_US", load = TRUE))
TextDocCol(ReutersSource(rcv1XML),
           readerControl = list(reader = readRCV1, language = "en_US", load = FALSE))
@

Or mails from newsgroups (as found in the \acronym{Uci} \acronym{Kdd} newsgroup data set):
<<>>=
# UCI KDD Newsgroup Mails
newsgroup <- system.file("texts", "newsgroup", package = "tm")

TextDocCol(DirSource(newsgroup),
           readerControl = list(reader = readNewsgroup, language = "en_US", load = TRUE))
@

\acronym{Rss} feed as delivered by Gmane for the \proglang{R} mailing list archive:
<<>>=
rss <- system.file("texts", "gmane.comp.lang.r.gr.rdf", package = "tm")

TextDocCol(GmaneSource(rss),
           readerControl = list(reader = readGmane, language = "en_US", load = FALSE))
@

For very simple \acronym{Html} documents:
<<>>=
html <- system.file("texts", "html", package = "tm")

TextDocCol(DirSource(html),
           readerControl = list(reader = readHTML, load = TRUE))
@

And for \acronym{Pdf} documents:
<<>>=
pdf <- system.file("texts", "pdf", package = "tm")

TextDocCol(DirSource(pdf),
           readerControl = list(reader = readPDF, language = "en_US", load = TRUE))
@
Note that \code{readPDF()} needs \code{pdftotext} and \code{pdfinfo}
installed on your system to be able to extract the text and meta
information from your \acronym{Pdf}s.

\section*{Inspecting the Text Document Collection}
Custom \code{show} and \code{summary} methods are available, which
hide the raw amount of information (consider a collection could
consists of several thousand documents, like a
database). \code{summary} gives more details on metadata than
\code{show}, whereas in order to actually see the content of text
documents use the command \code{inspect} on a collection.
<<>>=
show(ovid)
summary(ovid)
inspect(ovid[1:2])
@

\section*{Transformations}
Once we have a text document collection one typically wants to modify
the documents in it, e.g., stemming, stopword removal, et cetera.  In
\pkg{tm}, all this functionality is subsumed into the concept of
\emph{transformation}s. Transformations are done via the \code{tmMap}
function which applies a function to all elements of the
collection. Basically, all transformations work on single text documents
and \code{tmMap} just applies them to all documents in a document
collection.

\subsection*{Loading Documents into Memory}
If the source objects supports load on demand, but the user has not
enforced the package to load the input content directly into memory,
this can be done manually via \code{loadDoc}. Normally it is not
necessary to call this explicitly, as other functions working on text
corpora trigger this function for not-loaded documents (the corpus is
automatically loaded if accessed via \code{[[}).
<<>>=
reut21578TDC <- tmMap(reut21578TDC, loadDoc)
@

\subsection*{Converting to Plaintext Documents}
The text document collection \code{reut21578TDC} contains documents
in XML format. We have no further use for the XML interna and just
want to work with the text content. This can be done by converting the
documents to plaintext documents. It is done by the generic
\code{asPlain}.
<<>>=
reut21578TDC <- tmMap(reut21578TDC, asPlain)
@

\subsection*{Eliminating Extra Whitespace}
Extra whitespace is eliminated by:
<<>>=
reut21578TDC <- tmMap(reut21578TDC, stripWhitespace)
@

\subsection*{Convert to Lower Case}
Conversion to lower case by:
<<>>=
reut21578TDC <- tmMap(reut21578TDC, tmTolower)
@

\subsection*{Remove Stopwords}
Removal of stopwords by:
<<>>=
reut21578TDC <- tmMap(reut21578TDC, removeWords, stopwords("english"))
@

\subsection*{Stemming}
Stemming is done by:
<<>>=
tmMap(reut21578TDC, stemDoc)
@

\section*{Filters}
Often it is of special interest to filter out documents satisfying given
properties. For this purpose the function \code{tmFilter} is
designated. It is possible to write custom filter functions, but for
most cases the default filter does its job: it integrates a minimal
query language to filter metadata. Statements in this query language
are statements as used for subsetting data frames.

E.g., the following statement filters out those documents having
\code{COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE} as their
heading and an \code{ID} equal to 10 (both are metadata slot
variables of the text document).
<<keep.source=TRUE>>=
query <- "identifier == '10' &
          heading == 'COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE'"
tmFilter(reut21578TDC, query)
@

There is also a full text search filter available which accepts regular
expressions:
<<>>=
tmFilter(reut21578TDC, FUN = searchFullText, "partnership", doclevel = TRUE)
@

\section*{Adding Data or Metadata}

Text documents or metadata can be added to text document collections
with \code{appendElem} and \code{appendMeta}, respectively. The text
document collection has two types of metadata: one is the metadata on
the document collection level (\code{cmeta}), the other is the metadata
related to the individual documents (e.g., clusterings) (\code{dmeta})
in form of a dataframe. For the method \code{appendElem} it is possible
to give a row of values in the dataframe for the added data element.
<<>>=
data(crude)
reut21578TDC <- appendElem(reut21578TDC, crude[[1]], 0)
reut21578TDC <- appendMeta(reut21578TDC, cmeta = list(test = c(1,2,3)), dmeta = list(cl1 = 1:11))
summary(reut21578TDC)
CMetaData(reut21578TDC)
DMetaData(reut21578TDC)
@

\section*{Removing Metadata}
The metadata of text document collections can be easily modified or
removed:
<<>>=
data(crude)
reut21578TDC <- removeMeta(reut21578TDC, cname = "test", dname = "cl1")
CMetaData(reut21578TDC)
DMetaData(reut21578TDC)
@

\section*{Operators}
Many standard operators and functions (\code{[}, \code{[<-}, \code{[[}, \code{[[<-},
  \code{c}, \code{length}, \code{lapply}, \code{sapply}) are available for text document
collections with semantics similar to standard \proglang{R}
routines.
E.g., \code{c} concatenates two (or more) text document
collections. Applied to several text documents it returns a text
document collection. The metadata is automatically updated, if text
document collections are concatenated (i.e., merged).

Note also the custom element-of operator---it checks whether a text
document is already in a text document collection (metadata is not
checked, only the corpus):
<<>>=
crude[[1]] %IN% reut21578TDC
crude[[2]] %IN% reut21578TDC
@

\section*{Keeping Track of Text Document Collections}
There is a mechanism available for managing text document
collections. It is called \class{TextRepository}. A typical use would
be to save different states of a text document collection. A
repository has metadata in list format which can be either set with
\code{appendElem} as additional argument (e.g., a date when a new
element is added), or directly with \code{appendMeta}.
<<>>=
data(acq)
repo <- TextRepository(reut21578TDC)
repo <- appendElem(repo, acq, list(modified = date()))
repo <- appendMeta(repo, list(moremeta = 5:10))
summary(repo)
RepoMetaData(repo)
summary(repo[[1]])
summary(repo[[2]])
@

\section*{Creating Term-Document Matrices}
A common approach in text mining is to create a term-document matrix
for given texts. In this package the class \class{TermDocMatrix}
handles sparse matrices for text document collections.
<<>>=
tdm <- TermDocMatrix(reut21578TDC)
Data(tdm)[1:8,150:155]
@

\section*{Operations on Term-Document Matrices}
Besides the fact that on the \code{Data} part of this matrix a huge amount of \proglang{R}
functions (like clustering, classifications, etc.) is possible, this
package brings some shortcuts. Consider we
want to find those terms that occur at least 5 times:
<<>>=
findFreqTerms(tdm, 5, Inf)
@
Or we want to find associations (i.e., terms which correlate) with at
least $0.97$ correlation for the term \code{crop}:
<<>>=
findAssocs(tdm, "crop", 0.97)
@
The function also accepts a matrix as first argument (which does not
inherit from a term-document matrix). This matrix is then interpreted
as a correlation matrix and directly used. With this approach
different correlation measures can be employed.

Term-document matrices tend to get very big already for normal sized
datasets. Therefore we provide a method to remove \emph{sparse} terms,
i.e., terms occurring only in very few documents. Normally, this
reduces the matrix dramatically without losing significant relations
inherent to the matrix:
<<>>=
removeSparseTerms(tdm, 0.4)
@
This function call removes those terms which have at least a 40
percentage of sparse (i.e., terms occurring 0 times in a document)
elements.

\section*{Dictionary}
A dictionary is a (multi-)set of strings. It is often used to represent
relevant terms in text mining. We provide a class \class{Dictionary}
implementing such a dictionary concept. It can be created via the
\code{Dictionary} constructor, e.g.,
<<>>=
(d <- Dictionary(c("dlrs", "crude", "oil")))
@
and may be passed over to the \code{TermDocMatrix} constructor. Then
the created matrix is tabulated against the dictionary, i.e., only
terms from the dictionary appear in the matrix. This allows to
restrict the dimension of the matrix a priori and to focus on specific
terms for distinct text mining contexts, e.g.,
<<>>=
tdmD <- TermDocMatrix(reut21578TDC, dictionary = d)
Data(tdmD)
@
You can also create a dictionary from a term-document matrix via
\code{createDictionary} holding all terms from the matrix e.g.,
<<>>=
createDictionary(tdm)[100:110]
@
\end{document}
